{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef7610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# ensure project root is on PYTHONPATH\n",
    "ROOT = os.path.abspath(\"..\")       # adjust if needed\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.insert(0, ROOT)\n",
    "\n",
    "from task_dataset import TaskDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3337c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set paths\n",
    "EMB_PATH = \"F:\\\\TML_model_stealing\\\\TML25_A2_39\\\\data\\\\public_embeddings_seed61437433_port9732.npz\"\n",
    "IMG_PATH = \"F:\\\\TML_model_stealing\\\\TML25_A2_39\\\\data\\\\ModelStealingPub.pt\"\n",
    "\n",
    "print(\"Loading from:\")\n",
    "print(\" - Embeddings:\", EMB_PATH)\n",
    "print(\" - Images:\", IMG_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340da9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d91242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the victim .npz\n",
    "data = np.load(EMB_PATH)\n",
    "indices = data[\"indices\"]\n",
    "reps = data[\"reps\"]\n",
    "seed = int(data[\"seed\"])\n",
    "port = int(data[\"port\"])\n",
    "\n",
    "print(\"Loaded embeddings:\", reps.shape)\n",
    "\n",
    "# Load the corresponding public image dataset (.pt)\n",
    "dataset = torch.load(IMG_PATH, weights_only=False)\n",
    "images  = [dataset.imgs[i] for i in indices]\n",
    "\n",
    "# Ensure length matches\n",
    "assert len(images) == reps.shape[0]\n",
    "print(\"Loaded matching images:\", len(images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a11f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 5 images and their first 20 embedding dims\n",
    "for i in range(5):\n",
    "    img = images[i]\n",
    "    emb = reps[i]\n",
    "\n",
    "    plt.figure(figsize=(4, 1.5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(emb[:20])\n",
    "    plt.title(\"Embedding dims 0–19\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d30b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Stolen Encoder Architecture\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class StolenEncoder(nn.Module):\n",
    "    def __init__(self, pretrained_backbone=False):\n",
    "        super().__init__()\n",
    "        # 1) Backbone\n",
    "        self.backbone = resnet18(pretrained=pretrained_backbone)\n",
    "        # Remove the original classification head\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        # 2) Projection head to get 1024-dim outputs\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B,3,32,32)\n",
    "        feat = self.backbone(x)     # (B,512)\n",
    "        rep  = self.proj(feat)      # (B,1024)\n",
    "        return rep\n",
    "\n",
    "# Quickly test shape\n",
    "model = StolenEncoder(pretrained_backbone=False)\n",
    "x = torch.randn(4, 3, 32, 32)\n",
    "y = model(x)\n",
    "print(\"➡️ StolenEncoder output shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893628ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Basic transforms to convert PIL → Tensor + normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "class StealingDataset(Dataset):\n",
    "    def __init__(self, images, embeddings, transform=None):\n",
    "        \"\"\"\n",
    "        images: list of PIL.Image\n",
    "        embeddings: numpy array (N,1024)\n",
    "        \"\"\"\n",
    "        self.images     = images\n",
    "        self.embeddings = torch.from_numpy(embeddings).float()\n",
    "        self.transform  = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        rep = self.embeddings[idx]\n",
    "        return img, rep\n",
    "\n",
    "\n",
    "# Instantiate dataset & dataloader\n",
    "dataset_ssl = StealingDataset(images, reps, transform=transform)\n",
    "loader_ssl  = DataLoader(dataset_ssl, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "print(\"Dataset size:\", len(dataset_ssl))\n",
    "# print(\"One batch shapes:\", next(iter(loader_ssl))[0].shape, next(iter(loader_ssl))[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bfd401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training components\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = StolenEncoder(pretrained_backbone=False).to(device)\n",
    "\n",
    "# Loss: MSE between student & victim embeddings\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# (Optional) learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1841554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for one epoch & log loss\n",
    "\n",
    "model.train()\n",
    "running_loss = 0.0\n",
    "\n",
    "for batch_idx, (imgs, target_reps) in enumerate(loader_ssl, 1):\n",
    "    imgs = imgs.to(device)                  # (B,3,32,32)\n",
    "    target_reps = target_reps.to(device)    # (B,1024)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    pred_reps = model(imgs)                 # (B,1024)\n",
    "    loss = criterion(pred_reps, target_reps)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    if batch_idx % 50 == 0:\n",
    "        print(f\"  [Batch {batch_idx}] Avg Loss: {running_loss / batch_idx:.4f}\")\n",
    "\n",
    "# Step the scheduler\n",
    "scheduler.step()\n",
    "\n",
    "epoch_loss = running_loss / len(loader_ssl)\n",
    "print(f\"→ Epoch Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f31f83",
   "metadata": {},
   "source": [
    "## SimCLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3f88b",
   "metadata": {},
   "source": [
    "#### Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71371555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Path to victim embeddings and image dataset\n",
    "EMB_PATH = \"F:\\\\TML_model_stealing\\\\TML25_A2_39\\\\data\\\\public_embeddings_seed61437433_port9732.npz\"\n",
    "IMG_PATH = \"F:\\\\TML_model_stealing\\\\TML25_A2_39\\\\data\\\\ModelStealingPub.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b7172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load victim embeddings and image indices\n",
    "data = np.load(EMB_PATH)\n",
    "reps = data[\"reps\"]       # shape: (N,1024)\n",
    "indices = data[\"indices\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd2f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset and extract relevant images\n",
    "full_dataset = torch.load(IMG_PATH, weights_only=False)\n",
    "images = [full_dataset.imgs[i] for i in indices]\n",
    "\n",
    "print(f\"Loaded {len(images)} images and {reps.shape[0]} victim embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c656f4",
   "metadata": {},
   "source": [
    "#### Define Augmentations and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimCLR augmentations (strong)\n",
    "simclr_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32, scale=(0.2, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883cbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class returning two views + victim rep\n",
    "class SimCLRDataset(Dataset):\n",
    "    def __init__(self, images, victim_reps, transform):\n",
    "        self.images = images\n",
    "        self.vreps = torch.from_numpy(victim_reps).float()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert(\"RGB\")\n",
    "        t1 = self.transform(img)\n",
    "        t2 = self.transform(img)\n",
    "        rep = self.vreps[idx]\n",
    "        return t1, t2, rep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddb168c",
   "metadata": {},
   "source": [
    "#### Visualize Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef104ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show original image and two augmented views\n",
    "img_example = images[10]\n",
    "aug1 = simclr_transform(img_example)\n",
    "aug2 = simclr_transform(img_example)\n",
    "\n",
    "# Convert tensors back to image format for display\n",
    "def deprocess(t):\n",
    "    t = t.clone().detach().permute(1,2,0)\n",
    "    t = t * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n",
    "    return t.clamp(0, 1).numpy()\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img_example)\n",
    "plt.title(\"Original\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(deprocess(aug1))\n",
    "plt.title(\"Aug View 1\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(deprocess(aug2))\n",
    "plt.title(\"Aug View 2\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09f3f5",
   "metadata": {},
   "source": [
    "#### Define Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f86f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "class StolenEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet18(pretrained=False)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de00333",
   "metadata": {},
   "source": [
    "#### Train SimCLR Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4235b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = StolenEncoder().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loader = DataLoader(SimCLRDataset(images, reps, simclr_transform), batch_size=64, shuffle=True)\n",
    "\n",
    "alpha = 0.5  # weight between victim alignment and self-consistency\n",
    "num_epochs = 10\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    for t1, t2, yv in loader:\n",
    "        t1, t2, yv = t1.to(device), t2.to(device), yv.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        o1 = model(t1)         # (B,1024)\n",
    "        o2 = model(t2)         # (B,1024)\n",
    "        loss_A = F.mse_loss(o1, yv)\n",
    "        loss_B = F.mse_loss(o2, o1.detach())\n",
    "        loss = alpha * loss_A + (1 - alpha) * loss_B\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "\n",
    "    avg = total / len(loader)\n",
    "    loss_history.append(avg)\n",
    "    print(f\"[Epoch {epoch}] Loss: {avg:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.plot(loss_history, marker='o')\n",
    "plt.title(\"SimCLR-style Stealing Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb0a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "ONNX_PATH = \"F:\\\\TML_model_stealing\\\\TML25_A2_39\\\\models\\\\model_Sim_CLR.onnx\"\n",
    "dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    ONNX_PATH,\n",
    "    input_names=[\"x\"],\n",
    "    output_names=[\"embedding\"],\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    do_constant_folding=True\n",
    ")\n",
    "\n",
    "print(f\"\\u2705 Exported SimCLR ONNX model to: {ONNX_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6064c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
